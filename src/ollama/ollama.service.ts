import { EmbeddingService } from 'src/ollama/embedding.service';
import { Injectable } from '@nestjs/common';
import ollama from 'ollama';
import { QdrantService } from 'src/qdrant/qdrant.service';

@Injectable()
export class OllamaService {
  constructor(
    private readonly embeddingService: EmbeddingService,
    private readonly qdrantService: QdrantService,
  ) {}

  async search(question: string) {
    console.time('ğŸ”¹ ì „ì²´ ê²€ìƒ‰');

    try {
      // 1. ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
      console.time('â±ï¸ ì„ë² ë”© ìƒì„±');
      const questionEmbedding =
        await this.embeddingService.generateEmbedding(question);
      console.timeEnd('â±ï¸ ì„ë² ë”© ìƒì„±');

      // 2. Qdrant ê²€ìƒ‰ - ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í›„ì²˜ë¦¬
      console.time('â±ï¸ Qdrant ê²€ìƒ‰');
      const results = await this.qdrantService.searchVector(
        'test',
        questionEmbedding,
        {
          limit: 10, // ë” ë§ì€ ì²­í¬ë¥¼ ê°€ì ¸ì™€ì„œ
          score_threshold: 0.65, // ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ì¤€ ì¶”ê°€
        },
      );
      console.timeEnd('â±ï¸ Qdrant ê²€ìƒ‰');

      console.log(results, 'result');

      // 3. ê²°ê³¼ í•„í„°ë§ ë° ì •ë ¬
      const filteredResults = results
        .filter((r) => r.score && r.score > 0.75) // ìœ ì‚¬ë„ ì ìˆ˜ê°€ ë†’ì€ ê²ƒë§Œ ì‚¬ìš©
        .sort((a, b) => (b.score || 0) - (a.score || 0)); // ë†’ì€ ì ìˆ˜ìˆœ ì •ë ¬

      // 4. í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
      const relevantChunks = filteredResults
        .map((r) => ({
          text: r.payload?.text as string,
          score: r.score || 0,
          chunkIndex: r.payload?.chunkIndex as number,
          totalChunks: r.payload?.totalChunks as number,
        }))
        .filter((chunk) => chunk.text);

      console.log(`ê²€ìƒ‰ëœ ê´€ë ¨ ì²­í¬: ${relevantChunks.length}ê°œ`);

      // 5. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ìµœì í™” - ì²­í¬ ìœ„ì¹˜ì— ë”°ë¼ ì¸ì ‘ ì²­í¬ë„ í¬í•¨
      const enhancedContext = this.buildEnhancedContext(relevantChunks);

      // 6. ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì‘ì„±
      const prompt = this.buildSearchPrompt(question, enhancedContext);

      // 7. LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
      console.time('â±ï¸ LLM ë‹µë³€ ìƒì„±');
      const answer = await this.generateResponse(prompt);
      console.timeEnd('â±ï¸ LLM ë‹µë³€ ìƒì„±');

      console.timeEnd('ğŸ”¹ ì „ì²´ ê²€ìƒ‰');
      return {
        success: true,
        message: answer,
        relevantChunksCount: relevantChunks.length,
      };
    } catch (error) {
      console.error('ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜:', error);
      return {
        success: false,
        message: 'ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
        error: error.message,
      };
    }
  }

  // ê²€ìƒ‰ ê²°ê³¼ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°œì„ í•˜ëŠ” í•¨ìˆ˜
  private buildEnhancedContext(
    chunks: Array<{
      text: string;
      score: number;
      chunkIndex: number;
      totalChunks: number;
    }>,
  ): string {
    // ì ìˆ˜ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ê³ , ì¤‘ë³µ ì œê±°
    const uniqueChunks = new Map<number, { text: string; score: number }>();

    for (const chunk of chunks) {
      // ê¸°ì¡´ ì²­í¬ ì¶”ê°€
      uniqueChunks.set(chunk.chunkIndex, {
        text: chunk.text,
        score: chunk.score,
      });

      // ì¸ì ‘ ì²­í¬ë„ í¬í•¨ (ì „í›„ ê° 1ê°œ)
      if (chunk.chunkIndex > 0) {
        // ì´ì „ ì²­í¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        const prevChunkIndex = chunk.chunkIndex - 1;
        if (!uniqueChunks.has(prevChunkIndex)) {
          // ì´ì „ ì²­í¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œë¡œëŠ” Qdrantì—ì„œ ì¡°íšŒ í•„ìš”í•  ìˆ˜ ìˆìŒ)
          // ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬
        }
      }
    }

    // ì ìˆ˜ ê¸°ì¤€ ì •ë ¬í•˜ì—¬ ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
    const sortedTexts = Array.from(uniqueChunks.values())
      .sort((a, b) => b.score - a.score)
      .slice(0, 5)
      .map((item) => item.text);

    return sortedTexts.join('\n\n');
  }

  // ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
  private buildSearchPrompt(question: string, context: string): string {
    return `
  ### ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­
  ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.
  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , "ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
  ë‹µë³€ì€ ê°„ê²°í•˜ë©´ì„œë„ ì •ë³´ê°€ í’ë¶€í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
  í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
  ### ë¬¸ì„œ ë‚´ìš©
  ${context}
  
  ### ì§ˆë¬¸
  ${question}
  
  ### ë‹µë³€
  `;
  }

  // LLM ì‘ë‹µ ìƒì„± í•¨ìˆ˜
  private async generateResponse(prompt: string) {
    try {
      // ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ë” ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš©
      const response = await ollama.generate({
        model: 'llama3', // ë˜ëŠ” ë” ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸
        prompt,
        stream: false,
        options: {
          temperature: 0.3, // ë” ê²°ì •ì ì¸ ì‘ë‹µ
          top_p: 0.85, // ë” ì •í™•í•œ ì‘ë‹µ
          num_predict: 1024, // ì¶©ë¶„í•œ í† í° ìˆ˜
        },
      });

      return response.response;
    } catch (error) {
      console.error('LLM ì‘ë‹µ ìƒì„± ì˜¤ë¥˜:', error);
      throw new Error('ì‘ë‹µ ìƒì„± ì‹¤íŒ¨');
    }
  }
}
